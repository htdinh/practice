{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial \n",
    "* Following the course [CS 20SI: Tensorflow for Deep Learning Research](http://cs20si.stanford.edu/)\n",
    "* Course GitHub: https://github.com/chiphuyen/stanford-tensorflow-tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d071576/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a graph\n",
    "Reference [What is a TensorFlow Session?](https://danijar.com/what-is-a-tensorflow-session/)\n",
    "* First two lines are not needed as TensorFlow would create\n",
    "* Three operations are defined, and associated with the TF Graph:\n",
    "    * variable\n",
    "    * initialize\n",
    "    * assign\n",
    "* It's like defining your program (compute graph) and running it (session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    variable = tf.Variable(42, name='foo')\n",
    "    initialize = tf.global_variables_initializer()\n",
    "    assign = variable.assign(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Computation in a Session\n",
    "* Value of the variable is only valid in a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(initialize)\n",
    "    sess.run(assign)\n",
    "    print(sess.run(variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* New session independent of the other declared sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(initialize)\n",
    "    print(sess.run(variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "x = tf.add(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"./graphs\", sess.graph)\n",
    "    print(sess.run(x))\n",
    "    print(sess.run(tf.mod(b, a)))\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constant 1d tensor\n",
    "a = tf.constant([6, 5], name='vector') # optional name is \"Vector\"\n",
    "\n",
    "# Create 2x2 tensor\n",
    "b = tf.constant([2, 3], name='tensor')\n",
    "\n",
    "mul_op = tf.multiply(a, b)\n",
    "\n",
    "c = tf.linspace(10.0, 15.0, 5, name='linear_space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 5]\n",
      "[2 3]\n",
      "[ 10.    11.25  12.5   13.75  15.  ]\n",
      "[12 15]\n",
      "[6 5]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(mul_op))\n",
    "    print(a.eval()) # Should print the values of a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node {\n",
      "  name: \"Const\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_INT32\n",
      "        tensor_shape {\n",
      "        }\n",
      "        int_val: 2\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"Const_1\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_INT32\n",
      "        tensor_shape {\n",
      "        }\n",
      "        int_val: 3\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"Add\"\n",
      "  op: \"Add\"\n",
      "  input: \"Const\"\n",
      "  input: \"Const_1\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"FloorMod\"\n",
      "  op: \"FloorMod\"\n",
      "  input: \"Const_1\"\n",
      "  input: \"Const\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"vector\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_INT32\n",
      "        tensor_shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        tensor_content: \"\\006\\000\\000\\000\\005\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"tensor\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_INT32\n",
      "        tensor_shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        tensor_content: \"\\002\\000\\000\\000\\003\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"Mul\"\n",
      "  op: \"Mul\"\n",
      "  input: \"vector\"\n",
      "  input: \"tensor\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"linear_space/start\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_FLOAT\n",
      "        tensor_shape {\n",
      "        }\n",
      "        float_val: 10.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"linear_space/stop\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_FLOAT\n",
      "        tensor_shape {\n",
      "        }\n",
      "        float_val: 15.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"linear_space/num\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_INT32\n",
      "        tensor_shape {\n",
      "        }\n",
      "        int_val: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"linear_space\"\n",
      "  op: \"LinSpace\"\n",
      "  input: \"linear_space/start\"\n",
      "  input: \"linear_space/stop\"\n",
      "  input: \"linear_space/num\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"Tidx\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"my_new_const\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_FLOAT\n",
      "        tensor_shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        tensor_content: \"\\000\\000\\200?\\000\\000\\000@\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "versions {\n",
      "  producer: 24\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "my_const = tf.constant([1.0, 2.0], name=\"my_new_const\") \n",
    "print(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "sess  =  tf.InteractiveSession()\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(6)\n",
    "c = tf.add(a,b)\n",
    "print(c.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression: data of theft vs fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 845.4825156075614\n",
      "Epoch 10: 520.227675642286\n",
      "Epoch 20: 514.658701695147\n",
      "Epoch 30: 509.4259295761585\n",
      "Epoch 40: 504.50930420983406\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXkQk4hT0IiIRp6AXEYk4Bb2ISMT9f8s+lyoLRNhIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110d37ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Simple linear regression example in TensorFlow\n",
    "This program tries to predict the number of thefts from \n",
    "the number of fire in the city of Chicago\n",
    "Author: Chip Huyen\n",
    "Prepared for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "cs20si.stanford.edu\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import xlrd\n",
    "\n",
    "import utils\n",
    "\n",
    "DATA_FILE = 'data/fire_theft.xls'\n",
    "\n",
    "# Step 1: read in data from the .xls file\n",
    "book = xlrd.open_workbook(DATA_FILE, encoding_override=\"utf-8\")\n",
    "sheet = book.sheet_by_index(0)\n",
    "data = np.asarray([sheet.row_values(i) for i in range(1, sheet.nrows)])\n",
    "# 43 rows with first row as header\n",
    "n_samples = sheet.nrows - 1\n",
    "\n",
    "# Step 2: create placeholders for input X (number of fire) and label Y (number of theft)\n",
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')\n",
    "\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "w = tf.Variable(0.0, name='weights')\n",
    "b = tf.Variable(0.0, name='bias')\n",
    "\n",
    "# Step 4: build model to predict Y\n",
    "Y_predicted = X * w + b \n",
    "\n",
    "# Step 5: use the square error as the loss function\n",
    "loss = tf.square(Y - Y_predicted, name='loss')\n",
    "# loss = utils.huber_loss(Y, Y_predicted)\n",
    "\n",
    "# Step 6: using gradient descent with learning rate of 0.001 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "# Why large learning rate lead to inf \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 7: initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    \n",
    "    writer = tf.summary.FileWriter('./graphs/linear_reg', sess.graph)\n",
    "    \n",
    "    # Step 8: train the model\n",
    "    for i in range(50): # train the model 100 epochs\n",
    "        total_loss = 0  # sum of losses accross all training examples\n",
    "        for x, y in data:\n",
    "            # Session runs train_op and fetch values of loss\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: x, Y:y}) # {placeholder: variable}\n",
    "            total_loss += l\n",
    "        if (i%10==0):\n",
    "            print('Epoch {0}: {1}'.format(i, total_loss/n_samples))  # average loss per training example\n",
    "\n",
    "    # close the writer when you're done using it\n",
    "    writer.close() \n",
    "    \n",
    "    # Step 9: output the values of w and b\n",
    "    w, b = sess.run([w, b]) \n",
    "\n",
    "# plot the results\n",
    "X, Y = data.T[0], data.T[1]\n",
    "plt.plot(X, Y, 'bo', label='Real data')\n",
    "plt.plot(X, X * w + b, 'r', label='Predicted data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take away from first full use of TF in prediction\n",
    "* Since each epoch goes through all of the training samples, there is no _\"statistical\"_ property in the loss. Those statistical property would be seen if only a random sample of the training pool to be computed. \n",
    "* Learning rate should neither be too large leading to overshooting, or two small leading to slow convergence. \n",
    "* Given a fixed learning rate, with more interations, the loss would reduce unless in special conditions.\n",
    "* Given a fixed number of epoches, in a batch learning, the error in first epoch is the same independent of learning rate. However, in SGD, changing the learning rate also changes the loss of SGD even at first epoch because it parameters get updates from iterations of training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST example in Minibatch setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data using TensorFlow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from  tensorflow.examples.tutorials.mnist  import  input_data\n",
    "MNIST = input_data.read_data_sets(\"/data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 10\n",
    "\n",
    "# Step 1: Read in data from folder\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('/data/mnist', one_hot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# Features are of the type float, and labels are of the type int\n",
    "X = tf.placeholder(tf.float32, shape = [batch_size, 784], name='X')\n",
    "Y = tf.placeholder(tf.float32, shape = [batch_size, 10], name='Y') # Should be integer? \n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# weights and biases are initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = X * w + b\n",
    "# shape of b depends on Y\n",
    "\n",
    "w = tf.Variable(tf.zeros([784, 10]), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name='bias')\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "# to get the probability distribution of possible label of the image\n",
    "# DO NOT DO SOFTMAX HERE\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy loss of the real labels with the softmax of logits\n",
    "# use the method:\n",
    "# tf.nn.softmax_cross_entropy_with_logits(logits, Y)\n",
    "# then use tf.reduce_mean to get the mean loss of the batch\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy)\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent to minimize loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.3666749327054946\n",
      "Average loss epoch 1: 0.29621066005427243\n",
      "Average loss epoch 2: 0.28272875214303844\n",
      "Average loss epoch 3: 0.2803582388447437\n",
      "Average loss epoch 4: 0.2767757410596023\n",
      "Average loss epoch 5: 0.27271417905261747\n",
      "Average loss epoch 6: 0.26808451341860223\n",
      "Average loss epoch 7: 0.2660506339374678\n",
      "Average loss epoch 8: 0.26481766101745735\n",
      "Average loss epoch 9: 0.2643077180553705\n",
      "Total time: 4.119529962539673 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9189\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # to visualize using TensorBoard\n",
    "    writer = tf.summary.FileWriter('./graphs/logistic_reg', sess.graph)\n",
    "\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    print('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "    \n",
    "    # test the model\n",
    "\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    # Check if the maximum argument is consistent with labels\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "\n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run([accuracy], feed_dict={X: X_batch, Y:Y_batch})  \n",
    "        total_correct_preds += int(accuracy_batch[0])\n",
    "        \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method next_batch in module tensorflow.contrib.learn.python.learn.datasets.mnist:\n",
      "\n",
      "next_batch(batch_size, fake_data=False, shuffle=True) method of tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet instance\n",
      "    Return the next `batch_size` examples from this data set.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mnist.test.next_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway \n",
    "* Softmax is taken as loss function. \n",
    "* Average loss epoch is considered after running all batches. \n",
    "    * It is ok that we leave some example not fulfilling a batch outside (very possible)\n",
    "* Session structure: \n",
    "    * We can reuse the model _logits_ and re-run with new data specified by the placeholder. They must live in the same session because w is meaningful within a session. \n",
    "    * Subtitute relevant values for placeholders.\n",
    "* Functions are now \"tensorflow function\", that encapsulates all trivial handling of a vector operation.\n",
    "    * Most often, functions are nested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
